import tensorflow as tf
import numpy as np
import os
from utils.loading import loadOmniglot
from utils.dataprep import partitionByClass, prepareBatch, invert_img, deinvert_img, resize_img, subtract_mean, \
    augment_by_rotations
from utils.cnn6 import getPrototypes, getDistances, computeLoss, runModel, encoder_cnn_noaffine, get_predicted_value
from utils.visualize import plot_encoded_data, visualize
import argparse

tf.compat.v1.disable_v2_behavior()
tf.compat.v1.disable_eager_execution()


parser = argparse.ArgumentParser()
parser.add_argument('--fuzz_data_dir', type=str, default="fuzzdata/")
parser.add_argument('--mode', type=str, default="inference") # training | testing
parser.add_argument('--restore_file', type=str, default="model_checkpoints/name_of_the_run_e5_i3.ckpt")
args = parser.parse_args()

print("Running with fuzz data dir (--fuzz_data_dir=..): ", args.fuzz_data_dir)

embed_dim = 64  # small version
# embed_dim = 512

inference_mode = False
# if None, then no restoration, otherwise loads saved model,
# !! NEED TO MANUALLY MODIFY STARTING EPOCH IN runModel FOR LEARNING RATE ANNEALING TO WORK!
restore_file = args.restore_file   #"model_checkpoints/name_of_the_run_e1_i3.ckpt"
if restore_file is not None and args.mode == "inference":
  inference_mode = True

N_classes = 0
N_classes_val = 0
folder = ""

#Whether we use parts of 12 classes for testing
#Or we split 12 classes into 6 vs 6 for validating.
train_on_12_classes = True
#Whether if we use realworld data
realworld_data = False

if train_on_12_classes and not inference_mode:
  N_classes = 12
  N_classes_val = 12
  if realworld_data:
    folder = "realworld/" #Samples generated by Fuzzer
  else:
    folder = "splitdata/" #Lab generated samples

elif not train_on_12_classes and not inference_mode:
  #训练时的类别总数
  #一个文件夹算一个。当你修改上面的limit的时候，记得别把这里写错了，它要和实际的类别数一样。
  N_classes = 6
  #验证时的类别总数
  N_classes_val = 6
  folder = "fulldata-group3/"  #For validating

elif inference_mode:
  #其实这里应该从已有文件夹/模型/数据库里读出来的，无所谓了，这里只是模拟
  N_classes = 12
  N_classes_val = 12
  train_data_folder = "splitdata/"
  fuzz_data_folder = args.fuzz_data_dir

shots = 50
#支撑集的数量，用于X-shot分类。就是few-shot分类器选择few是多少的那个值（是这样吗？）。
N_support = 5  # number of support images per class = k of the k-shot classification
#查询集的数量，等于文件夹文件总数（一般是20张图）减去N_support。
# 注意！！上下这俩support+query加起来要小于文件总数。
N_query = shots - N_support  # number of query images, on Omniglot N_query + N_support <= 20

SIZE = 16*1024

#轮数
epochs = 10

dropout = 0.00
print_every = 1
plot_every = 1

#每隔几轮validate一下。
val_every = 10000000  # in epochs

np.random.seed(123)

# train 0 = full, 1 = subset, 2 = subset
# limit is limiting the number of loaded images for faster prototyping
# for actual training, go for train = 0, limit = None, that will load the full training data

#分别是：
#  训练集label、训练集图片数据、训练集标签的完整数据（例如images_background_small1 Balinese character02，这三个组成的三元集）；
#  验证集label、验证集图片数据、验证集标签的完整数据；

# TODO：
# DEL: 注意这里的limit限制了一共会读多少文件，正式使用时改为None
# LIMIT now limits how many files in one folder(class) will be read.

if not inference_mode:
  labels_train, images_train, info_train, labels_val, images_val, info_val = loadOmniglot(
      path=folder, train=1, train_limit=200, val_limit=200
  )

  # resizing and inverting (so that the letter is 1 and background 0)
  X_train = np.reshape(images_train, [-1, SIZE])
  X_val = np.reshape(images_val, [-1, SIZE])

  #X_train = invert_img(X_train)
  #X_val = invert_img(X_val)

  y_train = labels_train
  y_val = labels_val

  #减去均值，用于更快训练
  #TODO: 对文本而言需要这个吗？
  X_train, train_means = subtract_mean(X_train)
  X_val, val_means = subtract_mean(X_val)

  print("X_train", X_train.shape)
  print("X_val", X_val.shape)

sigma_mode_dict = {
    0: "constant",
    1: "radius",
    2: "diagonal",
    3: "full"  # not fully implemented, as not very useful
}

sigma_mode = sigma_mode_dict[1]

# visualize(deprocess_img(X_batch_support), output = "test.png", width = 5)

# defining TF model
tf.compat.v1.reset_default_graph()

# placeholders
X_support_ph = tf.compat.v1.placeholder(tf.float32, [None, SIZE])
X_query_ph = tf.compat.v1.placeholder(tf.float32, [None, SIZE])
y_support_ph = tf.compat.v1.placeholder(tf.int64, [None])
y_query_ph = tf.compat.v1.placeholder(tf.int64, [None])
class_ids_ph = tf.compat.v1.placeholder(tf.int64, [N_classes])

learning_rate_ph = tf.compat.v1.placeholder(tf.float32)  # for changing learning rate during training
is_training = tf.compat.v1.placeholder(tf.bool)
keep_prob = tf.compat.v1.placeholder(tf.float32)

encoder = encoder_cnn_noaffine  # choosen from utils/cnn6.py

print("======================================")
print("Processing support network encoder ....")
print("======================================")
print("")

with tf.compat.v1.variable_scope("support") as support_scope:
    X_support_encoded, sigma_support = encoder(X_support_ph, 1.0 - dropout, True, embed_dim,
                                               sigma_mode, SIZE)
print("======================================")
print("Processing query network encoder ....")
print("======================================")
print("")

with tf.compat.v1.variable_scope(support_scope, reuse=True):
    X_query_encoded, sigma_query = encoder(X_query_ph, 1.0 - dropout, True, embed_dim, sigma_mode, SIZE)

# normalizing to a circle -- not useful
# X_support_encoded = X_support_encoded / tf.norm(X_support_encoded, axis = 1, keep_dims = True)
# X_query_encoded = X_query_encoded / tf.norm(X_query_encoded, axis = 1, keep_dims = True)

print("======================================")
print("Get prototypes ....")
print("======================================")
print("")

prototypes, devs = getPrototypes(X_support_encoded, y_support_ph, class_ids_ph, N_classes=N_classes,
                                 sigma_support=sigma_support, sigma_mode=sigma_mode)
print("prototypes", prototypes)

print("======================================")
print("Get distances ....")
print("======================================")
print("")

distances, distances_normed = getDistances(X_query_encoded, y_query_ph, prototypes, devs, class_ids_ph,
                                           sigma_mode=sigma_mode)
print("distances", distances)

print("======================================")
print("Compute losses ....")
print("======================================")
print("")

loss, labels_predicted = computeLoss(y_query_ph, distances_normed, class_ids_ph, N_classes)
print("loss", loss)

optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate_ph)

extra_update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(extra_update_ops):
    train_step = optimizer.minimize(loss)

# defining the TF session
sess = tf.compat.v1.Session()
sess.run(tf.compat.v1.global_variables_initializer())

# Notes for running:
# -if not training, set training = None, other training = train_step
# -lr_halve sets how many epochs until learning rate is halved

print("======================================")
print("Run Model ....")
print("======================================")
print("")

# training = train_step

if not inference_mode:
  X_query_encoded_out = runModel(sess, labels_predicted, loss, class_ids_ph, X_train, y_train, X_support_ph, X_query_ph,
                               y_support_ph, y_query_ph, is_training, keep_prob, learning_rate_ph, X_val, y_val,
                               distances, dropout=dropout,
                               epochs=epochs, N_classes=N_classes, N_classes_val=N_classes_val, N_support=N_support,
                               N_query=N_query, print_every=print_every, plot_every=plot_every, val_every=val_every,
                               training=train_step, X_query_encoded=X_query_encoded,
                               checkpoint_name="name_of_the_run", support_scope=support_scope, encoder=encoder,
                               embed_dim=embed_dim, lr_start=1e-3, lr_halve=20 * 100, sigma_support=sigma_support,
                               sigma_mode=sigma_mode,
                               sigma_query=sigma_query, restore_file=restore_file, SIZE=SIZE, par_max_shots = shots)

#训完了以后呢？怎么测试单个样本的类别？？？
else:
  get_predicted_value(sess, labels_predicted, class_ids_ph, X_support_ph, X_query_ph,
                               y_support_ph, y_query_ph, is_training, keep_prob, learning_rate_ph,
                               distances, N_classes=N_classes, N_classes_val=N_classes_val, N_support=N_support,
                               N_query=N_query,
                               support_scope=support_scope, encoder=encoder,
                               embed_dim=embed_dim, sigma_support=sigma_support,
                               sigma_mode=sigma_mode,
                               restore_file=restore_file, SIZE=SIZE, par_max_shots = shots, train_data_folder = train_data_folder,
                               fuzz_data_folder = fuzz_data_folder)
#似乎会报错。。。先注释掉，不管这个。。
#plot_encoded_data(X_query_encoded_out, None, None, N_classes)
